import numpy as np
from sklearn.metrics import (
    mean_squared_error,
    mean_absolute_error,
    r2_score,
    mean_absolute_percentage_error,
    explained_variance_score
)

# Example actual and predicted values
actual = np.array([3.0, -0.5, 2.0, 7.0])
predicted = np.array([2.5, 0.0, 2.0, 8.0])

def evaluate_model(y_true, y_pred):
    print("ğŸ” Model Evaluation Metrics with Explanations\n")

    # 1ï¸âƒ£ Mean Squared Error (MSE)
    mse = mean_squared_error(y_true, y_pred)
    print(f"ğŸ“ Mean Squared Error (MSE): {mse:.4f}")
    print("    â†’ Formula: MSE = (1/n) * Î£(y_pred - y_actual)Â²")
    print("    â†’ Use: Penalizes large errors more heavily (squared). Useful in regression.")
    print("    â†’ Lower is better.\n")

    # 2ï¸âƒ£ Root Mean Squared Error (RMSE)
    rmse = np.sqrt(mse)
    print(f"ğŸ“ Root Mean Squared Error (RMSE): {rmse:.4f}")
    print("    â†’ Formula: RMSE = âˆšMSE")
    print("    â†’ Use: Similar to MSE, but in same units as target. Easier to interpret.")
    print("    â†’ Lower is better.\n")

    # 3ï¸âƒ£ Mean Absolute Error (MAE)
    mae = mean_absolute_error(y_true, y_pred)
    print(f"ğŸ“ Mean Absolute Error (MAE): {mae:.4f}")
    print("    â†’ Formula: MAE = (1/n) * Î£|y_pred - y_actual|")
    print("    â†’ Use: More robust to outliers than MSE. Easier to interpret.")
    print("    â†’ Lower is better.\n")

    # 4ï¸âƒ£ RÂ² Score (Coefficient of Determination)
    r2 = r2_score(y_true, y_pred)
    print(f"ğŸ“ RÂ² Score: {r2:.4f}")
    print("    â†’ Formula: RÂ² = 1 - (SS_res / SS_tot)")
    print("    â†’ Use: Measures % of variance explained by model.")
    print("    â†’ 1.0 is perfect, 0.0 means no better than mean, negative means worse.\n")

    # 5ï¸âƒ£ Mean Absolute Percentage Error (MAPE)
    mape = mean_absolute_percentage_error(y_true, y_pred)
    print(f"ğŸ“ Mean Absolute Percentage Error (MAPE): {mape:.4%}")
    print("    â†’ Formula: MAPE = (1/n) * Î£(|y_pred - y_actual| / |y_actual|)")
    print("    â†’ Use: Good when you care about relative (%) error.")
    print("    â†’ Lower % is better. Sensitive to small true values (div/zero risk).\n")

    # 6ï¸âƒ£ Explained Variance Score
    evs = explained_variance_score(y_true, y_pred)
    print(f"ğŸ“ Explained Variance Score: {evs:.4f}")
    print("    â†’ Formula: 1 - Var(residuals) / Var(actual)")
    print("    â†’ Use: Measures how well predictions capture variability.")
    print("    â†’ Close to 1 means model explains most of the variance.\n")

    # Return dictionary (optional)
    return {
        "MSE": mse,
        "RMSE": rmse,
        "MAE": mae,
        "R2": r2,
        "MAPE": mape,
        "Explained Variance": evs
    }

if __name__ == "__main__":
    metrics = evaluate_model(actual, predicted)